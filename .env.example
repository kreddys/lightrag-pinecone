# Directory Configuration
WORKING_DIR=/app/data/rag_storage
INPUT_DIR=/app/data/inputs

# LLM Configuration (Use valid host. For local services, you can use host.docker.internal)
# Ollama example
LLM_BINDING=openai
LLM_BINDING_HOST=https://api.deepseek.com/v1
LLM_MODEL=deepseek-chat
LLM_BINDING_API_KEY=

# Pinecone API Key (used for embeddings)
# Hardcoded to multilingual-e5-large
PINECONE_API_KEY=

# RAG Configuration
MAX_ASYNC=4
MAX_TOKENS=32768
EMBEDDING_DIM=1024
MAX_EMBED_TOKENS=507

# Logging
LOG_LEVEL=INFO
SENTRY_DSN=your-sentry-dsn-here
SENTRY_ENVIRONMENT=development  # or production